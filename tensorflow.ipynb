{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Projet de reconnaissance faciale",
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Reconnaissance faciale avec TensorFlow",
   "id": "d1e20c36a28590cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preparation des données",
   "id": "756af7cc97ff0381"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T10:56:55.077479Z",
     "start_time": "2024-12-13T10:56:55.071693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#import tensorflow as tf\n",
    "#from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "#\n",
    "## Charger le dataset\n",
    "#dataset_path = \"dataset\"\n",
    "#batch_size = 32\n",
    "#img_height = 224\n",
    "#img_width = 224\n",
    "#\n",
    "#train_dataset = image_dataset_from_directory(\n",
    "#    dataset_path,\n",
    "#    validation_split=0.2,\n",
    "#    subset=\"training\",\n",
    "#    seed=123,\n",
    "#    image_size=(img_height, img_width),\n",
    "#    batch_size=batch_size\n",
    "#)\n",
    "#\n",
    "#val_dataset = image_dataset_from_directory(\n",
    "#    dataset_path,\n",
    "#    validation_split=0.2,\n",
    "#    subset=\"validation\",\n",
    "#    seed=123,\n",
    "#    image_size=(img_height, img_width),\n",
    "#    batch_size=batch_size\n",
    "#)\n",
    "#\n",
    "## Récupérer les noms de classes avant la transformation\n",
    "#class_names = train_dataset.class_names\n",
    "#print(\"Class names:\", class_names)\n",
    "#\n",
    "## Normaliser les images\n",
    "#normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "#\n",
    "## Appliquer la normalisation\n",
    "#train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "#val_dataset = val_dataset.map(lambda x, y: (normalization_layer(x), y))"
   ],
   "id": "f266f9c5361a1395",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T10:56:55.334583Z",
     "start_time": "2024-12-13T10:56:55.117103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Paramètres\n",
    "dataset_path = \"dataset\"\n",
    "batch_size = 32\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "\n",
    "# Fonction pour organiser les images par angles avec diagnostic\n",
    "def organize_images_by_angles(class_path):\n",
    "    angles = {\n",
    "        'face': [],\n",
    "        'left': [],\n",
    "        'right': [],\n",
    "        'up': [],\n",
    "        'down': []\n",
    "    }\n",
    "\n",
    "    for img_file in os.listdir(class_path):\n",
    "        # Extraire le numéro de l'image\n",
    "        try:\n",
    "            # Utiliser une expression régulière plus flexible\n",
    "            match = re.search(r'face_(\\d+)', img_file)\n",
    "            if match:\n",
    "                num = int(match.group(1))\n",
    "                full_path = os.path.join(class_path, img_file)\n",
    "\n",
    "                if 0 <= num <= 4:\n",
    "                    angles['face'].append(full_path)\n",
    "                elif 5 <= num <= 9:\n",
    "                    angles['left'].append(full_path)\n",
    "                elif 10 <= num <= 14:\n",
    "                    angles['right'].append(full_path)\n",
    "                elif 15 <= num <= 19:\n",
    "                    angles['up'].append(full_path)\n",
    "                elif 20 <= num <= 24:\n",
    "                    angles['down'].append(full_path)\n",
    "        except (ValueError, TypeError):\n",
    "            # Ignorer les fichiers qui ne correspondent pas au motif\n",
    "            continue\n",
    "\n",
    "    return angles\n",
    "\n",
    "# Récupérer les noms de classes\n",
    "class_names = os.listdir(dataset_path)\n",
    "class_names = [nom for nom in class_names if os.path.isdir(os.path.join(dataset_path, nom))]\n",
    "\n",
    "# Préparation des données\n",
    "images = []\n",
    "labels = []\n",
    "image_paths = []\n",
    "\n",
    "for classe, nom in enumerate(class_names):\n",
    "    class_path = os.path.join(dataset_path, nom)\n",
    "\n",
    "    # Organiser les images par angles\n",
    "    angles_images = organize_images_by_angles(class_path)\n",
    "\n",
    "    # Vérifier que chaque catégorie a des images\n",
    "    val_images = []\n",
    "    for angle in ['face', 'left', 'right', 'up', 'down']:\n",
    "        if angles_images[angle]:\n",
    "            val_images.append(np.random.choice(angles_images[angle], 1)[0])\n",
    "        else:\n",
    "            print(f\"ATTENTION : Pas d'image pour l'angle {angle} dans {nom}\")\n",
    "\n",
    "    # Vérifier qu'on a bien des images de validation\n",
    "    if not val_images:\n",
    "        print(f\"ERREUR : Aucune image de validation pour {nom}\")\n",
    "        continue  # Passer à la classe suivante\n",
    "\n",
    "    # Charger les images de validation\n",
    "    for val_path in val_images:\n",
    "        try:\n",
    "            img = image.load_img(val_path, target_size=(img_height, img_width))\n",
    "            img_array = image.img_to_array(img)\n",
    "\n",
    "            images.append(img_array)\n",
    "            labels.append(classe)\n",
    "            image_paths.append(val_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du chargement de l'image {val_path}: {e}\")\n",
    "\n",
    "    # Charger les images d'entraînement (les autres)\n",
    "    train_images = [\n",
    "        path for angle_list in angles_images.values() for path in angle_list\n",
    "        if path not in val_images\n",
    "    ]\n",
    "\n",
    "    for train_path in train_images:\n",
    "        try:\n",
    "            img = image.load_img(train_path, target_size=(img_height, img_width))\n",
    "            img_array = image.img_to_array(img)\n",
    "\n",
    "            images.append(img_array)\n",
    "            labels.append(classe)\n",
    "            image_paths.append(train_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du chargement de l'image {train_path}: {e}\")\n",
    "\n",
    "# Convertir en numpy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "image_paths = np.array(image_paths)\n",
    "\n",
    "# Normalisation\n",
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "# Création des datasets TensorFlow\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "train_dataset = (train_dataset\n",
    "    .batch(batch_size)\n",
    "    .map(lambda x, y: (normalization_layer(x), y))\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# Créer le dataset de validation en prenant 1 image sur 6 (les images de validation)\n",
    "val_images = images[::6]\n",
    "val_labels = labels[::6]\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
    "val_dataset = (val_dataset\n",
    "    .batch(batch_size)\n",
    "    .map(lambda x, y: (normalization_layer(x), y))\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# Afficher les fichiers de validation\n",
    "print(\"\\nFichiers de validation :\")\n",
    "for classe in range(len(class_names)):\n",
    "    print(f\"\\nClasse {class_names[classe]} :\")\n",
    "    classe_val_paths = image_paths[(labels == classe) & (np.array([path in image_paths[labels == classe][0:5] for path in image_paths]))]\n",
    "    for path in classe_val_paths:\n",
    "        print(os.path.basename(path))\n",
    "\n",
    "# Autres informations de distribution\n",
    "print(\"\\nNoms des classes:\", class_names)\n",
    "print(\"Nombre total d'images:\", len(images))\n",
    "print(\"Nombre d'images par classe:\", len(images) // len(class_names))\n",
    "print(\"\\nNombre d'images d'entraînement:\", len(train_dataset) * batch_size)\n",
    "print(\"Nombre d'images de validation:\", len(val_dataset) * batch_size)"
   ],
   "id": "fc87dcec66e875cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fichiers de validation :\n",
      "\n",
      "Classe Edwin_Treny :\n",
      "face_3.jpg\n",
      "face_8.jpg\n",
      "face_10.jpg\n",
      "face_16.jpg\n",
      "face_22.jpg\n",
      "\n",
      "Classe Franck_Tene :\n",
      "face_4.jpg\n",
      "face_8.jpg\n",
      "face_14.jpg\n",
      "face_18.jpg\n",
      "face_23.jpg\n",
      "\n",
      "Classe Mathis_Wauters :\n",
      "face_0.jpg\n",
      "face_7.jpg\n",
      "face_14.jpg\n",
      "face_19.jpg\n",
      "face_23.jpg\n",
      "\n",
      "Classe Samuel_Jarjanette :\n",
      "face_1.jpg\n",
      "face_6.jpg\n",
      "face_13.jpg\n",
      "face_16.jpg\n",
      "face_24.jpg\n",
      "\n",
      "Noms des classes: ['Edwin_Treny', 'Franck_Tene', 'Mathis_Wauters', 'Samuel_Jarjanette']\n",
      "Nombre total d'images: 100\n",
      "Nombre d'images par classe: 25\n",
      "\n",
      "Nombre d'images d'entraînement: 128\n",
      "Nombre d'images de validation: 32\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Définir le modèle",
   "id": "9cd10270e791f848"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T10:56:55.958178Z",
     "start_time": "2024-12-13T10:56:55.378411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "# Charger MobileNetV2 sans la dernière couche\n",
    "base_model = MobileNetV2(input_shape=(img_height, img_width, 3),\n",
    "                         include_top=False,\n",
    "                         weights='imagenet')\n",
    "\n",
    "# Geler les couches du modèle pré-entraîné\n",
    "base_model.trainable = False\n",
    "\n",
    "# Ajouter des couches personnalisées\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(len(class_names), activation='softmax')  # Utiliser class_names\n",
    "])\n",
    "\n",
    "# Compiler le modèle\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Résumé du modèle\n",
    "model.summary()"
   ],
   "id": "8557d0d07b1da295",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_2\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_224            │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m7\u001B[0m, \u001B[38;5;34m7\u001B[0m, \u001B[38;5;34m1280\u001B[0m)     │     \u001B[38;5;34m2,257,984\u001B[0m │\n",
       "│ (\u001B[38;5;33mFunctional\u001B[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_1      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1280\u001B[0m)           │             \u001B[38;5;34m0\u001B[0m │\n",
       "│ (\u001B[38;5;33mGlobalAveragePooling2D\u001B[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)            │       \u001B[38;5;34m163,968\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001B[38;5;33mDropout\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)            │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m4\u001B[0m)              │           \u001B[38;5;34m516\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">163,968</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m2,422,468\u001B[0m (9.24 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,422,468</span> (9.24 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m164,484\u001B[0m (642.52 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">164,484</span> (642.52 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m2,257,984\u001B[0m (8.61 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Entraîner le modèle",
   "id": "cbfb4c5f4d4665e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T10:57:17.491010Z",
     "start_time": "2024-12-13T10:56:56.000446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs = 10\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs\n",
    ")\n"
   ],
   "id": "a37e0873af2c1d16",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 796ms/step - accuracy: 0.3182 - loss: 2.5096 - val_accuracy: 0.8235 - val_loss: 0.7154\n",
      "Epoch 2/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 415ms/step - accuracy: 0.7014 - loss: 0.7877 - val_accuracy: 0.4706 - val_loss: 1.0222\n",
      "Epoch 3/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 379ms/step - accuracy: 0.4963 - loss: 1.3433 - val_accuracy: 0.8824 - val_loss: 0.4670\n",
      "Epoch 4/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 378ms/step - accuracy: 0.7346 - loss: 0.6590 - val_accuracy: 1.0000 - val_loss: 0.1290\n",
      "Epoch 5/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 374ms/step - accuracy: 0.9479 - loss: 0.2409 - val_accuracy: 1.0000 - val_loss: 0.0798\n",
      "Epoch 6/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 392ms/step - accuracy: 0.9939 - loss: 0.1551 - val_accuracy: 1.0000 - val_loss: 0.0592\n",
      "Epoch 7/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 393ms/step - accuracy: 0.9661 - loss: 0.1345 - val_accuracy: 1.0000 - val_loss: 0.0386\n",
      "Epoch 8/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 392ms/step - accuracy: 0.9847 - loss: 0.0716 - val_accuracy: 1.0000 - val_loss: 0.0273\n",
      "Epoch 9/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 411ms/step - accuracy: 1.0000 - loss: 0.0673 - val_accuracy: 1.0000 - val_loss: 0.0229\n",
      "Epoch 10/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 376ms/step - accuracy: 0.9908 - loss: 0.0537 - val_accuracy: 1.0000 - val_loss: 0.0198\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Évaluer le modèle et ajuster les hyperparamètres",
   "id": "536aae76f87c12fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T10:57:41.818997Z",
     "start_time": "2024-12-13T10:57:17.511707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tester le modèle sur des données de validation\n",
    "val_loss, val_acc = model.evaluate(val_dataset)\n",
    "print(f\"Validation accuracy: {val_acc:.2f}\")\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "    ])\n",
    "\n",
    "# Appliquer la data augmentation\n",
    "train_dataset = train_dataset.map(lambda x, y: (data_augmentation(x), y))\n"
   ],
   "id": "4cdfe373eff20630",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 259ms/step - accuracy: 1.0000 - loss: 0.0198\n",
      "Validation accuracy: 1.00\n",
      "Epoch 1/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 510ms/step - accuracy: 0.7101 - loss: 0.7156 - val_accuracy: 1.0000 - val_loss: 0.0148\n",
      "Epoch 2/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 476ms/step - accuracy: 0.8893 - loss: 0.2966 - val_accuracy: 1.0000 - val_loss: 0.0374\n",
      "Epoch 3/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 485ms/step - accuracy: 0.9360 - loss: 0.3307 - val_accuracy: 1.0000 - val_loss: 0.0260\n",
      "Epoch 4/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 479ms/step - accuracy: 0.9327 - loss: 0.2054 - val_accuracy: 1.0000 - val_loss: 0.0102\n",
      "Epoch 5/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 478ms/step - accuracy: 0.8765 - loss: 0.2585 - val_accuracy: 1.0000 - val_loss: 0.0087\n",
      "Epoch 6/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 520ms/step - accuracy: 0.9630 - loss: 0.1422 - val_accuracy: 1.0000 - val_loss: 0.0098\n",
      "Epoch 7/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 543ms/step - accuracy: 0.9661 - loss: 0.1317 - val_accuracy: 1.0000 - val_loss: 0.0114\n",
      "Epoch 8/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 557ms/step - accuracy: 0.9847 - loss: 0.1148 - val_accuracy: 1.0000 - val_loss: 0.0101\n",
      "Epoch 9/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 564ms/step - accuracy: 0.9785 - loss: 0.0846 - val_accuracy: 1.0000 - val_loss: 0.0086\n",
      "Epoch 10/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 538ms/step - accuracy: 0.9908 - loss: 0.0457 - val_accuracy: 1.0000 - val_loss: 0.0063\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 246ms/step - accuracy: 1.0000 - loss: 0.0063\n",
      "Validation accuracy: 1.00\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sauvegarder le modèle",
   "id": "f823528f5cc618fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T10:57:42.209888Z",
     "start_time": "2024-12-13T10:57:41.838608Z"
    }
   },
   "cell_type": "code",
   "source": "model.save(\"face_recognition_model.keras\")\n",
   "id": "48b10aaf41d1c4f8",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reconnaissance faciale en temps réel",
   "id": "324625ae3a205697"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T10:57:46.538985Z",
     "start_time": "2024-12-13T10:57:42.238631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "import os\n",
    "import tkinter as tk\n",
    "from tkinter import Tk, Label\n",
    "from tkinter import simpledialog\n",
    "from tkinter import messagebox\n",
    "\n",
    "# Charger le modèle sauvegardé\n",
    "model = tf.keras.models.load_model(\"face_recognition_model.keras\")\n",
    "\n",
    "# Initialiser le classificateur de visage d'OpenCV\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Dimensions des images attendues\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "\n",
    "# Fonction pour effectuer une prédiction\n",
    "def predict_face(face, model, class_names):\n",
    "    face = cv2.resize(face, (img_width, img_height))  # Redimensionner\n",
    "    face_array = img_to_array(face) / 255.0  # Normaliser\n",
    "    face_array = np.expand_dims(face_array, axis=0)  # Ajouter une dimension batch\n",
    "    predictions = model.predict(face_array)\n",
    "    predicted_class = class_names[np.argmax(predictions)]\n",
    "    confidence = np.max(predictions)\n",
    "    return predicted_class, confidence\n",
    "\n",
    "def show_message(message, duration=5):\n",
    "    \"\"\"\n",
    "    Affiche un message dans une fenêtre pendant une durée spécifiée.\n",
    "    \"\"\"\n",
    "    root = Tk()\n",
    "    root.title(\"Prochaine étape\")\n",
    "    root.geometry(\"300x100\")\n",
    "    label = Label(root, text=message, font=(\"Helvetica\", 16), wraplength=280)\n",
    "    label.pack(expand=True)\n",
    "    root.after(duration * 1000, root.destroy)  # Ferme la fenêtre après `duration` secondes\n",
    "    root.mainloop()\n",
    "\n",
    "def capture_face(prenom, nom):\n",
    "    # Créer le dossier pour enregistrer les images si il n'existe pas\n",
    "    folder_name = f\"dataset/{prenom}_{nom}\"\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    # Ouvrir la caméra\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    captured_images = 0\n",
    "    directions = [\"Regardez a gauche\", \"Regardez a droite\", \"Regardez vers le haut\", \"Regardez vers le bas\", \"Regardez droit devant\"]\n",
    "    images_per_direction = 5  # Nombre d'images par direction\n",
    "    total_images = len(directions) * images_per_direction\n",
    "\n",
    "    direction_index = 0\n",
    "\n",
    "    while captured_images < total_images:\n",
    "        # Afficher le texte de l'étape actuelle\n",
    "        current_direction = directions[direction_index]\n",
    "        print(f\"Etape : {current_direction}\")\n",
    "\n",
    "        # Pause de 3 secondes avec affichage de l'étape actuelle\n",
    "        for i in range(3, 0, -1):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            # Ajouter le texte pour guider l'utilisateur\n",
    "            cv2.putText(frame, f\"{current_direction} dans {i} secondes\",\n",
    "                        (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "            cv2.imshow('Capturer le visage', frame)\n",
    "            cv2.waitKey(1000)\n",
    "\n",
    "        # Capturer les 50 images pour cette étape\n",
    "        while captured_images < (direction_index + 1) * images_per_direction:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Convertir en niveaux de gris pour la détection\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Détection des visages\n",
    "            faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "            for (x, y, w, h) in faces:\n",
    "                # Extraire le visage\n",
    "                face = frame[y:y+h, x:x+w]\n",
    "\n",
    "                # Enregistrer l'image\n",
    "                face_filename = f\"{folder_name}/face_{captured_images}.jpg\"\n",
    "                cv2.imwrite(face_filename, face)\n",
    "                captured_images += 1\n",
    "\n",
    "                # Ajouter un rectangle et des informations à l'écran\n",
    "                cv2.putText(frame, f\"Image {captured_images}/{total_images}\",\n",
    "                            (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "            # Afficher l'étape actuelle pendant la capture\n",
    "            cv2.putText(frame, f\"Etape actuelle : {current_direction}\",\n",
    "                        (10, frame.shape[0] - 20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)\n",
    "\n",
    "            # Afficher le flux vidéo avec les visages détectés\n",
    "            cv2.imshow('Capturer le visage', frame)\n",
    "\n",
    "            # Quitter avec la touche 'q'\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                return\n",
    "\n",
    "        # Passer à la prochaine direction\n",
    "        direction_index = (direction_index + 1) % len(directions)\n",
    "\n",
    "    # Libérer les ressources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    show_message(f\"Enregistrement terminé. Les visages ont été enregistrés dans le dossier : {folder_name}\")\n",
    "\n",
    "# Fonction pour analyser les visages\n",
    "def analyze_face():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Dictionnaire pour stocker les données des visages détectés\n",
    "    face_data = {}  # Clé : ID de visage, Valeur : {classe, score de confiance}\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convertir en niveaux de gris pour la détection\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Détection des visages\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "        # Mettre à jour les données des visages\n",
    "        new_face_data = {}\n",
    "        for i, (x, y, w, h) in enumerate(faces):\n",
    "            face_id = f\"face_{i}\"  # ID unique pour chaque visage détecté\n",
    "            face = frame[y:y+h, x:x+w]\n",
    "\n",
    "            # Prédire la classe et la confiance pour le visage actuel\n",
    "            predicted_class, confidence = predict_face(face, model, class_names)\n",
    "\n",
    "            # Si le visage est nouveau ou si la confiance est meilleure, mettre à jour\n",
    "            if face_id not in face_data or confidence > face_data[face_id]['confidence']:\n",
    "                new_face_data[face_id] = {'class': predicted_class, 'confidence': confidence}\n",
    "            else:\n",
    "                # Sinon, conserver les données précédentes\n",
    "                new_face_data[face_id] = face_data[face_id]\n",
    "\n",
    "            # Dessiner un rectangle autour du visage et ajouter le texte\n",
    "            color = (0, 255, 0)  # Vert pour le rectangle\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "            text = f\"{new_face_data[face_id]['class']} ({new_face_data[face_id]['confidence']:.2f})\"\n",
    "            cv2.putText(frame, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "\n",
    "        # Mettre à jour les données des visages\n",
    "        face_data = new_face_data\n",
    "\n",
    "        # Afficher le flux vidéo avec les prédictions\n",
    "        cv2.imshow('Reconnaissance Faciale', frame)\n",
    "\n",
    "        # Quitter avec la touche 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Libérer les ressources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# Interface graphique avec Tkinter\n",
    "def create_interface():\n",
    "    # Fenêtre principale\n",
    "    window = tk.Tk()\n",
    "    window.title(\"Reconnaissance Faciale\")\n",
    "\n",
    "    # Fonction pour enregistrer un visage\n",
    "    def start_capture():\n",
    "        prenom = simpledialog.askstring(\"Prénom\", \"Entrez votre prénom:\")\n",
    "        nom = simpledialog.askstring(\"Nom\", \"Entrez votre nom:\")\n",
    "        if prenom and nom:\n",
    "            capture_face(prenom, nom)\n",
    "\n",
    "    # Fonction pour analyser les visages\n",
    "    def start_analyze():\n",
    "        analyze_face()\n",
    "\n",
    "    # Boutons pour chaque fonctionnalité\n",
    "    capture_button = tk.Button(window, text=\"Enregistrer un visage\", command=start_capture)\n",
    "    capture_button.pack(pady=10)\n",
    "\n",
    "    analyze_button = tk.Button(window, text=\"Analyser un visage\", command=start_analyze)\n",
    "    analyze_button.pack(pady=10)\n",
    "\n",
    "    # Lancer l'interface\n",
    "    window.mainloop()\n",
    "\n",
    "# Lancer l'interface graphique\n",
    "create_interface()\n"
   ],
   "id": "d63a558e3a569f1d",
   "outputs": [],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
