{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Projet de reconnaissance faciale",
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Reconnaissance faciale avec TensorFlow",
   "id": "d1e20c36a28590cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preparation des données",
   "id": "756af7cc97ff0381"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T08:10:26.436044Z",
     "start_time": "2024-12-13T08:10:26.276468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "# Charger le dataset\n",
    "dataset_path = \"dataset\"\n",
    "batch_size = 32\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_dataset = image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Récupérer les noms de classes avant la transformation\n",
    "class_names = train_dataset.class_names\n",
    "print(\"Class names:\", class_names)\n",
    "\n",
    "# Normaliser les images\n",
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "# Appliquer la normalisation\n",
    "train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "val_dataset = val_dataset.map(lambda x, y: (normalization_layer(x), y))"
   ],
   "id": "f266f9c5361a1395",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75 files belonging to 3 classes.\n",
      "Using 60 files for training.\n",
      "Found 75 files belonging to 3 classes.\n",
      "Using 15 files for validation.\n",
      "Class names: ['Edwin_Treny', 'Mathis_Wauters', 'Samuel_Jarjanette']\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T08:10:26.454038Z",
     "start_time": "2024-12-13T08:10:26.441487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#import os\n",
    "#import numpy as np\n",
    "#import tensorflow as tf\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from tensorflow.keras.preprocessing import image\n",
    "#\n",
    "## Paramètres\n",
    "#dataset_path = \"dataset\"\n",
    "#batch_size = 32\n",
    "#img_height = 224\n",
    "#img_width = 224\n",
    "#\n",
    "## Récupérer les noms de classes\n",
    "#class_names = os.listdir(dataset_path)\n",
    "#class_names = [nom for nom in class_names if os.path.isdir(os.path.join(dataset_path, nom))]\n",
    "#\n",
    "## Charger manuellement les données\n",
    "#images = []\n",
    "#labels = []\n",
    "#image_paths = []  # Nouvelle liste pour stocker les chemins des images\n",
    "#\n",
    "#for classe, nom in enumerate(class_names):\n",
    "#    class_path = os.path.join(dataset_path, nom)\n",
    "#    for img_file in os.listdir(class_path):\n",
    "#        img_path = os.path.join(class_path, img_file)\n",
    "#\n",
    "#        # Charger et redimensionner l'image\n",
    "#        img = image.load_img(img_path, target_size=(img_height, img_width))\n",
    "#        img_array = image.img_to_array(img)\n",
    "#\n",
    "#        images.append(img_array)\n",
    "#        labels.append(classe)\n",
    "#        image_paths.append(img_path)  # Ajouter le chemin complet de l'image\n",
    "#\n",
    "## Convertir en numpy arrays\n",
    "#images = np.array(images)\n",
    "#labels = np.array(labels)\n",
    "#image_paths = np.array(image_paths)\n",
    "#\n",
    "## Split stratifié\n",
    "#X_train, X_val, y_train, y_val, paths_train, paths_val = train_test_split(\n",
    "#    images, labels, image_paths,\n",
    "#    test_size=0.2,\n",
    "#    stratify=labels,\n",
    "#    random_state=123\n",
    "#)\n",
    "#\n",
    "## Normalisation\n",
    "#normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "#\n",
    "## Création des datasets TensorFlow\n",
    "#train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "#val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "#\n",
    "## Configuration des datasets\n",
    "#train_dataset = (train_dataset\n",
    "#    .batch(batch_size)\n",
    "#    .map(lambda x, y: (normalization_layer(x), y))\n",
    "#    .prefetch(tf.data.AUTOTUNE)\n",
    "#)\n",
    "#\n",
    "#val_dataset = (val_dataset\n",
    "#    .batch(batch_size)\n",
    "#    .map(lambda x, y: (normalization_layer(x), y))\n",
    "#    .prefetch(tf.data.AUTOTUNE)\n",
    "#)\n",
    "#\n",
    "## Afficher les noms des fichiers de validation\n",
    "#print(\"\\nFichiers de validation :\")\n",
    "#for classe in range(len(class_names)):\n",
    "#    print(f\"\\nClasse {class_names[classe]} :\")\n",
    "#    classe_val_paths = paths_val[y_val == classe]\n",
    "#    for path in classe_val_paths:\n",
    "#        print(os.path.basename(path))\n",
    "#\n",
    "## Autres informations de distribution\n",
    "#print(\"\\nNoms des classes:\", class_names)\n",
    "#print(\"Nombre total d'images:\", len(images))\n",
    "#print(\"Images d'entraînement:\", len(X_train))\n",
    "#print(\"Images de validation:\", len(X_val))\n",
    "#\n",
    "## Vérification de la distribution des classes\n",
    "#print(\"\\nDistribution des classes totale:\")\n",
    "#unique, counts = np.unique(labels, return_counts=True)\n",
    "#for classe, count in zip(unique, counts):\n",
    "#    print(f\"{class_names[classe]}: {count}\")\n",
    "#\n",
    "#print(\"\\nDistribution des classes dans l'entraînement:\")\n",
    "#unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "#for classe, count in zip(unique_train, counts_train):\n",
    "#    print(f\"{class_names[classe]}: {count}\")\n",
    "#\n",
    "#print(\"\\nDistribution des classes dans la validation:\")\n",
    "#unique_val, counts_val = np.unique(y_val, return_counts=True)\n",
    "#for classe, count in zip(unique_val, counts_val):\n",
    "#    print(f\"{class_names[classe]}: {count}\")"
   ],
   "id": "fc87dcec66e875cb",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Définir le modèle",
   "id": "9cd10270e791f848"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T08:10:27.447603Z",
     "start_time": "2024-12-13T08:10:26.523309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "# Charger MobileNetV2 sans la dernière couche\n",
    "base_model = MobileNetV2(input_shape=(img_height, img_width, 3),\n",
    "                         include_top=False,\n",
    "                         weights='imagenet')\n",
    "\n",
    "# Geler les couches du modèle pré-entraîné\n",
    "base_model.trainable = False\n",
    "\n",
    "# Ajouter des couches personnalisées\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(len(class_names), activation='softmax')  # Utiliser class_names\n",
    "])\n",
    "\n",
    "# Compiler le modèle\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Résumé du modèle\n",
    "model.summary()"
   ],
   "id": "8557d0d07b1da295",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_2\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_224            │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m7\u001B[0m, \u001B[38;5;34m7\u001B[0m, \u001B[38;5;34m1280\u001B[0m)     │     \u001B[38;5;34m2,257,984\u001B[0m │\n",
       "│ (\u001B[38;5;33mFunctional\u001B[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_1      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1280\u001B[0m)           │             \u001B[38;5;34m0\u001B[0m │\n",
       "│ (\u001B[38;5;33mGlobalAveragePooling2D\u001B[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)            │       \u001B[38;5;34m163,968\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001B[38;5;33mDropout\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)            │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m3\u001B[0m)              │           \u001B[38;5;34m387\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">163,968</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m2,422,339\u001B[0m (9.24 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,422,339</span> (9.24 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m164,355\u001B[0m (642.01 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">164,355</span> (642.01 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m2,257,984\u001B[0m (8.61 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Entraîner le modèle",
   "id": "cbfb4c5f4d4665e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T08:10:47.896261Z",
     "start_time": "2024-12-13T08:10:27.475022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs = 10\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs\n",
    ")\n"
   ],
   "id": "a37e0873af2c1d16",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 3s/step - accuracy: 0.3701 - loss: 1.3959 - val_accuracy: 1.0000 - val_loss: 0.3591\n",
      "Epoch 2/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 773ms/step - accuracy: 0.7396 - loss: 0.6021 - val_accuracy: 0.9333 - val_loss: 0.2562\n",
      "Epoch 3/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 817ms/step - accuracy: 0.9007 - loss: 0.3560 - val_accuracy: 1.0000 - val_loss: 0.1213\n",
      "Epoch 4/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 747ms/step - accuracy: 0.9458 - loss: 0.1371 - val_accuracy: 1.0000 - val_loss: 0.0656\n",
      "Epoch 5/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 699ms/step - accuracy: 0.9778 - loss: 0.1236 - val_accuracy: 1.0000 - val_loss: 0.0471\n",
      "Epoch 6/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 785ms/step - accuracy: 0.9674 - loss: 0.1379 - val_accuracy: 1.0000 - val_loss: 0.0449\n",
      "Epoch 7/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 729ms/step - accuracy: 0.9569 - loss: 0.0720 - val_accuracy: 1.0000 - val_loss: 0.0571\n",
      "Epoch 8/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 741ms/step - accuracy: 0.9889 - loss: 0.0401 - val_accuracy: 1.0000 - val_loss: 0.0612\n",
      "Epoch 9/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 717ms/step - accuracy: 1.0000 - loss: 0.0213 - val_accuracy: 1.0000 - val_loss: 0.0466\n",
      "Epoch 10/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 739ms/step - accuracy: 1.0000 - loss: 0.0257 - val_accuracy: 1.0000 - val_loss: 0.0287\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Évaluer le modèle et ajuster les hyperparamètres",
   "id": "536aae76f87c12fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T08:11:08.109458Z",
     "start_time": "2024-12-13T08:10:47.931254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tester le modèle sur des données de validation\n",
    "val_loss, val_acc = model.evaluate(val_dataset)\n",
    "print(f\"Validation accuracy: {val_acc:.2f}\")\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "    ])\n",
    "\n",
    "# Appliquer la data augmentation\n",
    "train_dataset = train_dataset.map(lambda x, y: (data_augmentation(x), y))\n",
    "\n",
    "# Ré-entraîner le modèle avec la data augmentation\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=epochs\n",
    ")\n",
    "\n",
    "# Tester le modèle sur des données de validation\n",
    "val_loss, val_acc = model.evaluate(val_dataset)\n",
    "print(f\"Validation accuracy: {val_acc:.2f}\")\n"
   ],
   "id": "4cdfe373eff20630",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 267ms/step - accuracy: 1.0000 - loss: 0.0287\n",
      "Validation accuracy: 1.00\n",
      "Epoch 1/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 930ms/step - accuracy: 0.8812 - loss: 0.3297 - val_accuracy: 1.0000 - val_loss: 0.0106\n",
      "Epoch 2/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 945ms/step - accuracy: 0.9028 - loss: 0.1771 - val_accuracy: 1.0000 - val_loss: 0.0313\n",
      "Epoch 3/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 917ms/step - accuracy: 0.9236 - loss: 0.1832 - val_accuracy: 0.9333 - val_loss: 0.0680\n",
      "Epoch 4/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 1s/step - accuracy: 0.8806 - loss: 0.2654 - val_accuracy: 1.0000 - val_loss: 0.0322\n",
      "Epoch 5/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 888ms/step - accuracy: 0.9569 - loss: 0.0994 - val_accuracy: 1.0000 - val_loss: 0.0153\n",
      "Epoch 6/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 884ms/step - accuracy: 0.9569 - loss: 0.1540 - val_accuracy: 1.0000 - val_loss: 0.0109\n",
      "Epoch 7/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 903ms/step - accuracy: 0.9674 - loss: 0.0891 - val_accuracy: 1.0000 - val_loss: 0.0098\n",
      "Epoch 8/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 908ms/step - accuracy: 0.9458 - loss: 0.1397 - val_accuracy: 1.0000 - val_loss: 0.0066\n",
      "Epoch 9/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 970ms/step - accuracy: 0.9569 - loss: 0.0968 - val_accuracy: 1.0000 - val_loss: 0.0039\n",
      "Epoch 10/10\n",
      "\u001B[1m2/2\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 1s/step - accuracy: 0.9785 - loss: 0.0480 - val_accuracy: 1.0000 - val_loss: 0.0036\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 342ms/step - accuracy: 1.0000 - loss: 0.0036\n",
      "Validation accuracy: 1.00\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sauvegarder le modèle",
   "id": "f823528f5cc618fb"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-13T08:11:08.141446Z"
    }
   },
   "cell_type": "code",
   "source": "model.save(\"face_recognition_model.keras\")\n",
   "id": "48b10aaf41d1c4f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reconnaissance faciale en temps réel",
   "id": "324625ae3a205697"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T08:09:49.124665Z",
     "start_time": "2024-12-13T08:08:23.330938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "import os\n",
    "import tkinter as tk\n",
    "from tkinter import Tk, Label\n",
    "from tkinter import simpledialog\n",
    "from tkinter import messagebox\n",
    "\n",
    "# Charger le modèle sauvegardé\n",
    "model = tf.keras.models.load_model(\"face_recognition_model.keras\")\n",
    "\n",
    "# Initialiser le classificateur de visage d'OpenCV\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Dimensions des images attendues\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "\n",
    "# Seuil de confiance en dessous duquel la personne est considérée comme non reconnue\n",
    "CONFIDENCE_THRESHOLD = 0.85\n",
    "\n",
    "# Fonction pour effectuer une prédiction\n",
    "def predict_face(face, model, class_names):\n",
    "    face = cv2.resize(face, (img_width, img_height))  # Redimensionner\n",
    "    face_array = img_to_array(face) / 255.0  # Normaliser\n",
    "    face_array = np.expand_dims(face_array, axis=0)  # Ajouter une dimension batch\n",
    "    predictions = model.predict(face_array)\n",
    "    predicted_class = class_names[np.argmax(predictions)]\n",
    "    confidence = np.max(predictions)\n",
    "    return predicted_class, confidence\n",
    "\n",
    "def show_message(message, duration=5):\n",
    "    \"\"\"\n",
    "    Affiche un message dans une fenêtre pendant une durée spécifiée.\n",
    "    \"\"\"\n",
    "    root = Tk()\n",
    "    root.title(\"Prochaine étape\")\n",
    "    root.geometry(\"300x100\")\n",
    "    label = Label(root, text=message, font=(\"Helvetica\", 16), wraplength=280)\n",
    "    label.pack(expand=True)\n",
    "    root.after(duration * 1000, root.destroy)  # Ferme la fenêtre après `duration` secondes\n",
    "    root.mainloop()\n",
    "\n",
    "def capture_face(prenom, nom):\n",
    "    # Créer le dossier pour enregistrer les images si il n'existe pas\n",
    "    folder_name = f\"dataset/{prenom}_{nom}\"\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    # Ouvrir la caméra\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    captured_images = 0\n",
    "    directions = [\"Regardez a gauche\", \"Regardez a droite\", \"Regardez vers le haut\", \"Regardez vers le bas\", \"Regardez droit devant\"]\n",
    "    images_per_direction = 5  # Nombre d'images par direction\n",
    "    total_images = len(directions) * images_per_direction\n",
    "\n",
    "    direction_index = 0\n",
    "\n",
    "    while captured_images < total_images:\n",
    "        # Afficher le texte de l'étape actuelle\n",
    "        current_direction = directions[direction_index]\n",
    "        print(f\"Etape : {current_direction}\")\n",
    "\n",
    "        # Pause de 3 secondes avec affichage de l'étape actuelle\n",
    "        for i in range(3, 0, -1):\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            # Ajouter le texte pour guider l'utilisateur\n",
    "            cv2.putText(frame, f\"{current_direction} dans {i} secondes\",\n",
    "                        (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "            cv2.imshow('Capturer le visage', frame)\n",
    "            cv2.waitKey(1000)\n",
    "\n",
    "        # Capturer les 50 images pour cette étape\n",
    "        while captured_images < (direction_index + 1) * images_per_direction:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Convertir en niveaux de gris pour la détection\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Détection des visages\n",
    "            faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "            for (x, y, w, h) in faces:\n",
    "                # Extraire le visage\n",
    "                face = frame[y:y+h, x:x+w]\n",
    "\n",
    "                # Enregistrer l'image\n",
    "                face_filename = f\"{folder_name}/face_{captured_images}.jpg\"\n",
    "                cv2.imwrite(face_filename, face)\n",
    "                captured_images += 1\n",
    "\n",
    "                # Ajouter un rectangle et des informations à l'écran\n",
    "                cv2.putText(frame, f\"Image {captured_images}/{total_images}\",\n",
    "                            (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "            # Afficher l'étape actuelle pendant la capture\n",
    "            cv2.putText(frame, f\"Etape actuelle : {current_direction}\",\n",
    "                        (10, frame.shape[0] - 20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)\n",
    "\n",
    "            # Afficher le flux vidéo avec les visages détectés\n",
    "            cv2.imshow('Capturer le visage', frame)\n",
    "\n",
    "            # Quitter avec la touche 'q'\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                return\n",
    "\n",
    "        # Passer à la prochaine direction\n",
    "        direction_index = (direction_index + 1) % len(directions)\n",
    "\n",
    "    # Libérer les ressources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    show_message(f\"Enregistrement terminé. Les visages ont été enregistrés dans le dossier : {folder_name}\")\n",
    "\n",
    "# Fonction pour analyser les visages\n",
    "def analyze_face():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convertir en niveaux de gris pour la détection\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Détection des visages\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            # Extraire le visage\n",
    "            face = frame[y:y+h, x:x+w]\n",
    "\n",
    "            # Prédire le nom\n",
    "            predicted_class, confidence = predict_face(face, model, class_names)\n",
    "\n",
    "            # Si la confiance est inférieure au seuil, afficher \"Inconnu\"\n",
    "            if confidence < CONFIDENCE_THRESHOLD:\n",
    "                color = (0, 0, 255)  # Rouge pour le rectangle\n",
    "                predicted_class = \"Inconnu\"\n",
    "                text = f\"{predicted_class} ({confidence*100:.1f}%)\"\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "\n",
    "            # Dessiner un rectangle autour du visage et ajouter le texte\n",
    "            color = (0, 255, 0)  # Vert pour le rectangle\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "            text = f\"{predicted_class} ({confidence*100:.1f}%)\"\n",
    "            cv2.putText(frame, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "\n",
    "        # Afficher le flux vidéo avec les prédictions\n",
    "        cv2.imshow('Reconnaissance Faciale', frame)\n",
    "\n",
    "        # Quitter avec la touche 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Libérer les ressources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Interface graphique avec Tkinter\n",
    "def create_interface():\n",
    "    # Fenêtre principale\n",
    "    window = tk.Tk()\n",
    "    window.title(\"Reconnaissance Faciale\")\n",
    "\n",
    "    # Fonction pour enregistrer un visage\n",
    "    def start_capture():\n",
    "        prenom = simpledialog.askstring(\"Prénom\", \"Entrez votre prénom:\")\n",
    "        nom = simpledialog.askstring(\"Nom\", \"Entrez votre nom:\")\n",
    "        if prenom and nom:\n",
    "            capture_face(prenom, nom)\n",
    "\n",
    "    # Fonction pour analyser les visages\n",
    "    def start_analyze():\n",
    "        analyze_face()\n",
    "\n",
    "    # Boutons pour chaque fonctionnalité\n",
    "    capture_button = tk.Button(window, text=\"Enregistrer un visage\", command=start_capture)\n",
    "    capture_button.pack(pady=10)\n",
    "\n",
    "    analyze_button = tk.Button(window, text=\"Analyser un visage\", command=start_analyze)\n",
    "    analyze_button.pack(pady=10)\n",
    "\n",
    "    # Lancer l'interface\n",
    "    window.mainloop()\n",
    "\n",
    "# Lancer l'interface graphique\n",
    "create_interface()\n"
   ],
   "id": "d63a558e3a569f1d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 2s/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 80ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 79ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 80ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 88ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 95ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 79ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 94ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 98ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 81ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 91ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 90ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 80ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 88ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 78ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 88ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 81ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 80ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 82ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 80ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 78ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 91ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 81ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 91ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 91ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 88ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 88ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 79ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 81ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 73ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 80ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 82ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 88ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 74ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 81ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 82ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 88ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 88ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 88ms/step\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
